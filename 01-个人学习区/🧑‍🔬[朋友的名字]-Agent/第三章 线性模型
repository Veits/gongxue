 ## 线性模型：基础与核心算法的数学解析

本章作为机器学习模型的开篇，强调了**线性模型**的基石地位，它是众多复杂模型（如支持向量机、神经网络）的底层组成单元。学习内容聚焦于**线性回归**、**对数几率回归**（逻辑回归）和**线性判别分析**。

### 1. 线性回归（Linear Regression）

线性回归用于解决**回归任务**（预测连续值），其核心思想是利用**最小二乘法**（Least Squares Method）来确定最优模型参数。

#### 1.1 优化目标：最小化误差平方和
模型的目标函数是找到一组最优参数 $\mathbf{w}$ 和 $b$，使预测值与真实值之间的**均方误差（MSE）**最小化：

$$(\mathbf{w}^* , b^*) = \arg\min_{(\mathbf{w}, b)} E(\mathbf{w}, b) = \arg\min_{(\mathbf{w}, b)} \sum_{i=1}^m (y_i - (\mathbf{w}^T \mathbf{x}_i + b))^2$$

其中，$\arg\min$ 表示求取使目标函数达到最小值的参数取值。

#### 1.2 求解方法：闭式解（解析解）
为了简化推导，将参数和特征向量进行增广，引入 $\mathbf{\hat{w}} = (\mathbf{w}; b)$ 和 $\mathbf{\hat{x}}_i = (\mathbf{x}_i; 1)$，目标函数可写为矩阵形式 $E_{\mathbf{\hat{w}}} = (\mathbf{y} - \mathbf{X}\mathbf{\hat{w}})^T (\mathbf{y} - \mathbf{X}\mathbf{\hat{w}})$.

通过对 $E_{\mathbf{\hat{w}}}$ 求**梯度** $\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}}$ 并令其为零，可得参数的**闭式解（解析解）**：

$$\mathbf{\hat{w}}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

这是一个重要的特例，因为多数机器学习模型没有闭式解。在Agent应用中，如果数据矩阵 $\mathbf{X}$ 满足 $\mathbf{X}^T \mathbf{X}$ 为**正定矩阵**（确保全局最优解存在），该方法提供了高效且准确的参数求解途径。若 $\mathbf{X}^T \mathbf{X}$ 不可逆，则需要引入**正则化**或使用**伪逆矩阵**求解。

### 2. 对数几率回归（Logistic Regression）

对数几率回归用于解决**分类任务**，它通过将线性回归的输出值映射到 $(0, 1)$ 区间，从而得到样本属于某一类别的**概率**。

#### 2.1 概率映射：Sigmoid 函数
模型使用**对数几率函数（Sigmoid function）**将线性预测值 $\mathbf{w}^T \mathbf{x} + b$ 转换为概率：

$$P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}$$

#### 2.2 优化目标：极大似然估计与凸函数
参数 $\mathbf{\beta}$ 的确定是通过最大化**似然函数**，该过程等价于最小化其相反数，即**损失函数** $\ell(\mathbf{\beta})$：

$$\min_{\mathbf{\beta}} \ell(\mathbf{\beta}) = \min_{\mathbf{\beta}} \sum_{i=1}^m \left( -y_i \mathbf{\beta}^T \mathbf{\hat{x}}_i + \ln(1 + e^{\mathbf{\beta}^T \mathbf{\hat{x}}_i}) \right)$$

一个关键的数学性质是，$\ell(\mathbf{\beta})$ 是关于 $\mathbf{\beta}$ 的**凸函数**。根据凸函数定理，这保证了通过优化算法找到的任何局部最优解都是**全局最优解**，这对Agent应用的训练稳定性和结果可靠性至关重要。

#### 2.3 求解方法：迭代优化
由于 $\ell(\mathbf{\beta})$ 没有闭式解，必须采用**迭代优化算法**进行求解：

* **梯度下降法（Gradient Descent）：** 每次迭代沿着目标函数梯度的**反方向**（最速下降方向）移动，步长由**学习率 $a$** 控制。
    $$\mathbf{x}_{t+1} = \mathbf{x}_t - a \nabla f(\mathbf{x}_t)$$
* **牛顿法（Newton's Method）：** 利用函数的**二阶泰勒展开**进行近似，每次迭代不仅使用梯度 $\nabla f(\mathbf{x}_t)$，还使用函数的二阶导数矩阵（**Hessian 矩阵** $\nabla^2 f(\mathbf{x}_t)$）的逆矩阵，收敛速度通常快于梯度下降法。
    $$\mathbf{x}_{t+1} = \mathbf{x}_t - [\nabla^2 f(\mathbf{x}_t)]^{-1} \nabla f(\mathbf{\mathbf{x}}_t)$$

在Agent应用实践中，通常采用计算复杂度较低的梯度下降法及其变种（如SGD、Adam等）。

### 3. 线性判别分析（Linear Discriminant Analysis, LDA）

LDA 是一种经典的**分类和监督降维**方法，它旨在找到一个投影方向 $\mathbf{w}$，使得：
1.  **类内散度（Within-class scatter）** 尽可能小：同类样本投影后尽可能聚集。
2.  **类间散度（Between-class scatter）** 尽可能大：异类样本投影后中心点尽可能远离。

#### 3.1 优化目标：最大化广义瑞利商
LDA 的目标是最大化一个度量值 $J$，即**广义瑞利商**：

$$J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_b \mathbf{w}}{\mathbf{w}^T \mathbf{S}_w \mathbf{w}}$$

其中，$\mathbf{S}_b$ 为**类间散度矩阵**，$\mathbf{S}_w$ 为**类内散度矩阵**。

#### 3.2 求解方法：广义特征值问题
通过拉格朗日乘子法，将优化问题转化为求解一个**广义特征值问题**：

$$\mathbf{S}_b \mathbf{w} = \lambda \mathbf{S}_w \mathbf{w}$$

在二分类问题中，最优解 $\mathbf{w}$ 的方向正比于 $\mathbf{S}_w^{-1} (\mathbf{\mu}_0 - \mathbf{\mu}_1)$。对于多分类问题，通过选择前 $N-1$ 个最大的**广义特征值**所对应的特征向量 $\mathbf{w}_i$ 来构成投影矩阵 $\mathbf{W}$，从而实现降维和分类。这在处理高维特征的Agent应用中具有重要价值。

 
## 1\. 线性回归 (Linear Regression) 实践

线性回归的实践重点是实现其两种求解方式：**闭式解（矩阵法）和梯度下降法（迭代法）**。

### 实践目标：使用矩阵法求解 $\mathbf{\hat{w}}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$

这里我们将使用 **NumPy** 来实现解析解。

```python
import numpy as np

# 1. 准备数据 (假设只有一维特征 x 和目标值 y)
X = np.array([1, 2, 3, 4, 5])  # 训练样本特征
y = np.array([2.1, 3.9, 6.2, 8.0, 9.8]) # 目标值

# 2. 特征增广：将特征 X 转换为矩阵 X_b，加入偏置项 b (截距项)
# 对应于理论中的 向量 x^i = (x_i; 1)
# 形式为: [[x1, 1], [x2, 1], ...]
X_b = np.c_[X, np.ones(X.shape[0])] 

# X_b 现在是 (m, 2) 维矩阵：
# [[1., 1.],
#  [2., 1.],
#  [3., 1.],
#  [4., 1.],
#  [5., 1.]]

# 3. 闭式解计算 (正规方程)
# w_hat = (X_b.T * X_b)^-1 * X_b.T * y
try:
    # np.linalg.inv() 求逆矩阵
    # @ 运算符在 NumPy 中表示矩阵乘法
    w_hat = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
    
    # w_hat[0] 是 w (权重), w_hat[1] 是 b (截距)
    print(f"解析解参数 w_hat: {w_hat}")
    print(f"权重 w (斜率): {w_hat[0]:.4f}")
    print(f"截距 b: {w_hat[1]:.4f}")
    
    # 预测
    X_new = np.array([6])
    X_new_b = np.c_[X_new, np.ones(X_new.shape[0])]
    y_predict = X_new_b @ w_hat
    print(f"预测 x=6 时的值: {y_predict[0]:.4f}")

except np.linalg.LinAlgError:
    print("矩阵 (X^T * X) 不可逆，无法使用闭式解。考虑使用梯度下降或伪逆。")
```

-----

## 2\. 对数几率回归 (Logistic Regression) 实践

对数几率回归没有闭式解，因此我们使用**迭代优化算法**进行求解。这里我们将使用 **Scikit-learn** 库，它默认采用优化算法（如梯度下降的变体）来最小化损失函数。

### 实践目标：使用 Scikit-learn 实现逻辑回归分类

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
import numpy as np

# 1. 准备数据：使用鸢尾花数据集的两个特征进行二分类
iris = load_iris()
X = iris.data[:100, [0, 1]] # 只取前100个样本（两类）和前两个特征
y = iris.target[:100]       # 目标值 y (0 或 1)

# 2. 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. 初始化模型
# solver='lbfgs' 是 Scikit-learn 默认的优化器，属于拟牛顿法（效率高于纯梯度下降）
# C=1.0 是正则化参数的倒数
log_reg = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)

# 4. 训练模型 (求解 beta 向量)
# 这一步内部就是通过迭代优化算法（如拟牛顿法）最小化损失函数
log_reg.fit(X_train, y_train)

# 5. 模型评估
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"模型训练准确率: {accuracy:.4f}")

# 6. 获取模型参数 (对应于理论中的 beta)
# log_reg.coef_ 对应于 w 向量
# log_reg.intercept_ 对应于 b 截距项
print(f"模型权重 w: {log_reg.coef_}")
print(f"模型截距 b: {log_reg.intercept_}")

# 7. 预测概率 (体现 sigmoid 映射)
# 预测第一个测试样本属于类别 1 的概率
proba = log_reg.predict_proba(X_test[0].reshape(1, -1))[0]
print(f"\n第一个测试样本属于类别 0/1 的概率: {proba}")
```

-----

## 3\. 线性判别分析 (LDA) 实践

LDA 主要用于**降维**，将高维数据投影到最佳的低维子空间，以最大化类间距离和最小化类内距离。

### 实践目标：使用 Scikit-learn 实现 LDA 降维

我们将使用 Iris 数据集，将其从 4 个特征降到 2 个特征。

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np

# 1. 准备数据 (使用所有类别和所有特征)
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# 2. 初始化 LDA 模型
# n_components=2 表示将数据降到 2 维。
# 注意：对于 N 个类别，LDA 最多只能降到 N-1 维。
lda = LDA(n_components=2)

# 3. 训练和转换 (求解最佳投影方向 W)
# 这一步完成了求解广义特征值问题 S_b * w = lambda * S_w * w
X_r = lda.fit(X, y).transform(X)

# X_r 现在是 (m, 2) 维矩阵

# 4. 结果可视化 (降维效果)
plt.figure()
colors = ['navy', 'turquoise', 'darkorange']

# 投影到 2 维后，不同类别样本的分布情况
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], alpha=.8, color=color,
                label=target_name)
    
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('LDA 降维至 2D')
plt.xlabel('LD1 (第一判别轴)')
plt.ylabel('LD2 (第二判别轴)')
plt.grid(True)
plt.show()

# 5. 判别轴（权重）信息
# 投影轴信息存储在 lda.scalings_ 中，展示了原始特征对判别方向的贡献
print("\n每个原始特征对判别轴的贡献 (相当于投影方向 W):")
print(lda.scalings_)
