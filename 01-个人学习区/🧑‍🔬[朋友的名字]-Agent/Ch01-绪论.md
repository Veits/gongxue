# Ch01-ç»ªè®º - Agentè§†è§’

## ğŸ§  æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µç†è§£

### å­¦ä¹ èŒƒå¼

#### ç›‘ç£å­¦ä¹  (Supervised Learning)
**æ•°å­¦å®šä¹‰**: ç»™å®šè®­ç»ƒæ•°æ®é›† $D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$ï¼Œå­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•° $f: X \rightarrow Y$ï¼Œä½¿å¾—å¯¹äºæ–°çš„è¾“å…¥ $x$ï¼Œèƒ½å¤Ÿé¢„æµ‹å¯¹åº”çš„è¾“å‡º $y$ã€‚

**ç®—æ³•åˆ†ç±»**:
- **åˆ†ç±»é—®é¢˜**: $Y$ æ˜¯ç¦»æ•£çš„ç±»åˆ«é›†åˆ
- **å›å½’é—®é¢˜**: $Y$ æ˜¯è¿ç»­çš„å®æ•°ç©ºé—´

**Agentåº”ç”¨æ€è€ƒ**: åœ¨Agentç³»ç»Ÿä¸­ï¼Œç›‘ç£å­¦ä¹ å¯ä»¥ç”¨äºï¼š
- æ„å›¾è¯†åˆ«ï¼šå°†ç”¨æˆ·è¾“å…¥æ˜ å°„åˆ°ç‰¹å®šæ„å›¾
- è¡Œä¸ºé¢„æµ‹ï¼šé¢„æµ‹ç¯å¢ƒä¸­çš„å…¶ä»–Agentè¡Œä¸º
- å†³ç­–ä¼˜åŒ–ï¼šå­¦ä¹ æœ€ä¼˜å†³ç­–å‡½æ•°

#### æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)
**æ•°å­¦å®šä¹‰**: ç»™å®šè®­ç»ƒæ•°æ®é›† $D = \{x_1, x_2, ..., x_n\}$ï¼Œå‘ç°æ•°æ®ä¸­çš„éšè—ç»“æ„æˆ–æ¨¡å¼ã€‚

**ç®—æ³•åˆ†ç±»**:
- **èšç±»**: å°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„
- **é™ç»´**: å‡å°‘ç‰¹å¾ç»´åº¦ï¼Œä¿ç•™é‡è¦ä¿¡æ¯
- **å¯†åº¦ä¼°è®¡**: ä¼°è®¡æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ

**Agentåº”ç”¨æ€è€ƒ**:
- ç¯å¢ƒæ¢ç´¢ï¼šè‡ªåŠ¨å‘ç°ç¯å¢ƒä¸­çš„é‡è¦åŒºåŸŸ
- çŠ¶æ€èšåˆï¼šå°†ç›¸ä¼¼çŠ¶æ€åˆå¹¶ï¼Œç®€åŒ–çŠ¶æ€ç©ºé—´
- å¼‚å¸¸æ£€æµ‹ï¼šè¯†åˆ«ç¯å¢ƒä¸­çš„å¼‚å¸¸æƒ…å†µ

#### å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)
**æ•°å­¦å®šä¹‰**: Agenté€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ $\pi^*$ï¼Œæœ€å¤§åŒ–æœŸæœ›å›æŠ¥ã€‚

**æ ¸å¿ƒè¦ç´ **:
- çŠ¶æ€ç©ºé—´ $S$
- åŠ¨ä½œç©ºé—´ $A$
- å¥–åŠ±å‡½æ•° $R: S \times A \rightarrow \mathbb{R}$
- ç­–ç•¥ $\pi: S \rightarrow A$

**Agentåº”ç”¨æ€è€ƒ**: è¿™æ˜¯Agentçš„æ ¸å¿ƒå­¦ä¹ èŒƒå¼
- å†³ç­–åˆ¶å®šï¼šå­¦ä¹ åœ¨ç‰¹å®šçŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œ
- å¤šAgentåä½œï¼šå¤šä¸ªAgentä¹‹é—´çš„åè°ƒ
- è‡ªé€‚åº”è¡Œä¸ºï¼šæ ¹æ®ç¯å¢ƒåé¦ˆè°ƒæ•´è¡Œä¸º

### åŸºæœ¬æœ¯è¯­çš„æ•°å­¦è¡¨è¾¾

#### ç‰¹å¾ç©ºé—´ (Feature Space)
- **è¾“å…¥ç©ºé—´**: $\mathcal{X} \subseteq \mathbb{R}^d$
- **ç‰¹å¾å‘é‡**: $\mathbf{x} = (x_1, x_2, ..., x_d)^T$
- **ç‰¹å¾æ˜ å°„**: $\phi: \mathcal{X} \rightarrow \mathcal{H}$

#### å‡è®¾ç©ºé—´ (Hypothesis Space)
- **å‡è®¾é›†åˆ**: $\mathcal{H} = \{h: \mathcal{X} \rightarrow \mathcal{Y}\}$
- **å‚æ•°åŒ–å‡è®¾**: $h_\theta: \mathcal{X} \rightarrow \mathcal{Y}$ï¼Œå…¶ä¸­ $\theta \in \Theta$

#### æŸå¤±å‡½æ•° (Loss Function)
- **0-1æŸå¤±**: $L(y, \hat{y}) = \mathbb{I}(y \neq \hat{y})$
- **å¹³æ–¹æŸå¤±**: $L(y, \hat{y}) = (y - \hat{y})^2$
- **ç»å¯¹æŸå¤±**: $L(y, \hat{y}) = |y - \hat{y}|$

#### ç»éªŒé£é™©æœ€å°åŒ– (ERM)
$$\hat{h} = \arg\min_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n L(y_i, h(x_i))$$

#### ç»“æ„é£é™©æœ€å°åŒ– (SRM)
$$\hat{h} = \arg\min_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n L(y_i, h(x_i)) + \lambda \Omega(h)$$

## âš™ï¸ ç®—æ³•å¤æ‚åº¦åˆ†æ

### æ—¶é—´å¤æ‚åº¦

#### çº¿æ€§å›å½’
- **è®­ç»ƒ**: $O(nd^2)$ï¼Œå…¶ä¸­ $n$ æ˜¯æ ·æœ¬æ•°ï¼Œ$d$ æ˜¯ç‰¹å¾æ•°
- **é¢„æµ‹**: $O(d)$
- **å†…å­˜**: $O(d^2)$

#### k-è¿‘é‚»
- **è®­ç»ƒ**: $O(1)$
- **é¢„æµ‹**: $O(nkd)$ï¼Œå…¶ä¸­ $k$ æ˜¯è¿‘é‚»æ•°
- **å†…å­˜**: $O(nd)$

#### å†³ç­–æ ‘
- **è®­ç»ƒ**: $O(n \log n \cdot d)$
- **é¢„æµ‹**: $O(\text{depth})$
- **å†…å­˜**: $O(\text{nodes})$

### ç©ºé—´å¤æ‚åº¦

#### æ¨¡å‹å­˜å‚¨
- **å‚æ•°æ¨¡å‹**: å­˜å‚¨å‚æ•° $\theta$
- **éå‚æ•°æ¨¡å‹**: å­˜å‚¨è®­ç»ƒæ•°æ®
- **æ··åˆæ¨¡å‹**: å­˜å‚¨å‚æ•°å’Œéƒ¨åˆ†æ•°æ®

#### è®¡ç®—ä¼˜åŒ–
- **å¢é‡å­¦ä¹ **: $O(1)$ å¢é‡æ›´æ–°
- **æ‰¹é‡å­¦ä¹ **: $O(n)$ æ‰¹é‡å¤„ç†
- **åœ¨çº¿å­¦ä¹ **: $O(1)$ åœ¨çº¿æ›´æ–°

## ğŸ¤– Agentç³»ç»Ÿä¸­çš„åº”ç”¨æ¶æ„

### æ„ŸçŸ¥å±‚ (Perception Layer)
```python
class PerceptionLayer:
    def __init__(self, feature_extractor):
        self.feature_extractor = feature_extractor
        self.state_encoder = StateEncoder()
    
    def perceive(self, raw_observation):
        # åŸå§‹è§‚å¯Ÿ â†’ ç‰¹å¾æå– â†’ çŠ¶æ€ç¼–ç 
        features = self.feature_extractor.extract(raw_observation)
        encoded_state = self.state_encoder.encode(features)
        return encoded_state
```

### å†³ç­–å±‚ (Decision Layer)
```python
class DecisionLayer:
    def __init__(self, policy_network):
        self.policy_network = policy_network
        self.value_network = ValueNetwork()
    
    def decide(self, state):
        # çŠ¶æ€ â†’ åŠ¨ä½œä»·å€¼ â†’ åŠ¨ä½œé€‰æ‹©
        action_values = self.policy_network(state)
        state_value = self.value_network(state)
        action = self.select_action(action_values)
        return action, state_value
```

### æ‰§è¡Œå±‚ (Execution Layer)
```python
class ExecutionLayer:
    def __init__(self, action_executor):
        self.action_executor = action_executor
        self.feedback_handler = FeedbackHandler()
    
    def execute(self, action):
        # åŠ¨ä½œæ‰§è¡Œ â†’ åé¦ˆæ¥æ”¶ â†’ ç»éªŒå­˜å‚¨
        result = self.action_executor.execute(action)
        feedback = self.feedback_handler.receive(result)
        return feedback
```

## ğŸ”§ å®ç°æŠ€æœ¯ç»†èŠ‚

### ç‰¹å¾å·¥ç¨‹

#### æ•°å€¼ç‰¹å¾
- **æ ‡å‡†åŒ–**: $x' = \frac{x - \mu}{\sigma}$
- **å½’ä¸€åŒ–**: $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$
- **åˆ†æ¡¶**: å°†è¿ç»­å€¼è½¬æ¢ä¸ºç¦»æ•£å€¼

#### ç±»åˆ«ç‰¹å¾
- **ç‹¬çƒ­ç¼–ç **: $\text{OneHot}(x_i) = [0, ..., 1, ..., 0]$
- **æ ‡ç­¾ç¼–ç **: $\text{Label}(x_i) \in \{0, 1, ..., k-1\}$
- **åµŒå…¥ç¼–ç **: $\text{Embedding}(x_i) \in \mathbb{R}^d$

#### æ–‡æœ¬ç‰¹å¾
- **è¯è¢‹æ¨¡å‹**: $\phi_{\text{BOW}}(d) = [\text{count}(w_1, d), ..., \text{count}(w_V, d)]$
- **TF-IDF**: $\text{TF-IDF}(w, d) = \text{TF}(w, d) \times \log\frac{N}{\text{DF}(w)}$
- **è¯åµŒå…¥**: $\text{Word2Vec}(w) \in \mathbb{R}^d$

### æ¨¡å‹é€‰æ‹©ç­–ç•¥

#### é—®é¢˜ç±»å‹åˆ†æ
```python
def select_model(task_type, data_size, feature_size):
    if task_type == "classification":
        if data_size < 1000:
            return LogisticRegression()
        elif feature_size < 100:
            return RandomForest()
        else:
            return NeuralNetwork()
    elif task_type == "regression":
        if data_size < 1000:
            return LinearRegression()
        else:
            return GradientBoosting()
```

#### è®¡ç®—å¤æ‚åº¦è€ƒè™‘
```python
def optimize_for_efficiency(model, constraints):
    if constraints["memory"] == "low":
        return compress_model(model)
    elif constraints["latency"] == "critical":
        return quantize_model(model)
    else:
        return model
```

### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

#### å¹¶è¡Œè®¡ç®—
- **æ•°æ®å¹¶è¡Œ**: å°†æ•°æ®åˆ†å—å¹¶è¡Œå¤„ç†
- **æ¨¡å‹å¹¶è¡Œ**: å°†æ¨¡å‹åˆ†åˆ°å¤šä¸ªè®¾å¤‡
- **æµæ°´çº¿å¹¶è¡Œ**: é‡å è®¡ç®—å’Œé€šä¿¡

#### å†…å­˜ä¼˜åŒ–
- **æ‡’åŠ è½½**: æŒ‰éœ€åŠ è½½æ•°æ®
- **å†…å­˜æ˜ å°„**: ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶
- **åƒåœ¾å›æ”¶**: åŠæ—¶é‡Šæ”¾æ— ç”¨å†…å­˜

#### ç¼“å­˜ä¼˜åŒ–
- **ç‰¹å¾ç¼“å­˜**: ç¼“å­˜è®¡ç®—å¯†é›†çš„ç‰¹å¾
- **æ¨¡å‹ç¼“å­˜**: ç¼“å­˜å¸¸ç”¨æ¨¡å‹
- **ç»“æœç¼“å­˜**: ç¼“å­˜é‡å¤è®¡ç®—

## ğŸ“Š æ€§èƒ½è¯„ä¼°æŒ‡æ ‡

### å‡†ç¡®æ€§æŒ‡æ ‡

#### åˆ†ç±»é—®é¢˜
- **å‡†ç¡®ç‡**: $\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$
- **ç²¾ç¡®ç‡**: $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$
- **å¬å›ç‡**: $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$
- **F1åˆ†æ•°**: $\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$

#### å›å½’é—®é¢˜
- **MSE**: $\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$
- **MAE**: $\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$
- **RÂ²**: $R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$

### æ•ˆç‡æŒ‡æ ‡

#### è®¡ç®—æ•ˆç‡
- **è®­ç»ƒæ—¶é—´**: å•æ¬¡è®­ç»ƒçš„æ—¶é—´æ¶ˆè€—
- **é¢„æµ‹æ—¶é—´**: å•æ¬¡é¢„æµ‹çš„æ—¶é—´æ¶ˆè€—
- **ååé‡**: å•ä½æ—¶é—´å¤„ç†çš„è¯·æ±‚æ•°

#### èµ„æºæ•ˆç‡
- **å†…å­˜ä½¿ç”¨**: è¿è¡Œæ—¶çš„å†…å­˜æ¶ˆè€—
- **CPUä½¿ç”¨**: CPUåˆ©ç”¨ç‡
- **GPUä½¿ç”¨**: GPUåˆ©ç”¨ç‡

## ğŸ› ï¸ å®é™…å®ç°è€ƒè™‘

### ä»£ç å®ç°ç»“æ„

#### æ•°æ®é¢„å¤„ç†
```python
class DataPreprocessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.encoder = OneHotEncoder()
        self.imputer = SimpleImputer()
    
    def fit(self, X, y=None):
        # æ‹Ÿåˆé¢„å¤„ç†å‚æ•°
        return self
    
    def transform(self, X):
        # åº”ç”¨é¢„å¤„ç†
        return X_processed
    
    def fit_transform(self, X, y=None):
        # æ‹Ÿåˆå¹¶åº”ç”¨é¢„å¤„ç†
        return X_processed
```

#### æ¨¡å‹è®­ç»ƒ
```python
class ModelTrainer:
    def __init__(self, model):
        self.model = model
        self.validator = CrossValidator()
    
    def train(self, X, y):
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X, y)
        return self.model
    
    def validate(self, X, y):
        # éªŒè¯æ¨¡å‹æ€§èƒ½
        scores = self.validator.cross_validate(self.model, X, y)
        return scores
```

#### æ¨¡å‹éƒ¨ç½²
```python
class ModelDeployer:
    def __init__(self, model):
        self.model = model
        self.preprocessor = DataPreprocessor()
    
    def predict(self, X):
        # é¢„å¤„ç†å¹¶é¢„æµ‹
        X_processed = self.preprocessor.transform(X)
        predictions = self.model.predict(X_processed)
        return predictions
    
    def save(self, path):
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self, path)
    
    @classmethod
    def load(cls, path):
        # åŠ è½½æ¨¡å‹
        return joblib.load(path)
```

### è°ƒè¯•å’Œæµ‹è¯•

#### å•å…ƒæµ‹è¯•
```python
class TestModelTrainer(unittest.TestCase):
    def setUp(self):
        self.trainer = ModelTrainer(LinearRegression())
        self.X, self.y = make_regression(n_samples=100, n_features=5)
    
    def test_training(self):
        model = self.trainer.train(self.X, self.y)
        self.assertIsNotNone(model)
    
    def test_validation(self):
        scores = self.trainer.validate(self.X, self.y)
        self.assertGreater(scores['test_score'].mean(), 0.5)
```

#### æ€§èƒ½æµ‹è¯•
```python
class TestModelPerformance(unittest.TestCase):
    def test_prediction_latency(self):
        model = ModelDeployer(LinearRegression())
        start_time = time.time()
        model.predict(self.X)
        latency = time.time() - start_time
        self.assertLess(latency, 0.1)
```

## ğŸ“ˆ Agentç³»ç»Ÿä¸­çš„å®é™…æŒ‘æˆ˜

### å®æ—¶æ€§è¦æ±‚
- **ä½å»¶è¿Ÿ**: Agentéœ€è¦å¿«é€Ÿå“åº”ç¯å¢ƒå˜åŒ–
- **é«˜åå**: å¤„ç†å¤§é‡çš„å®æ—¶æ•°æ®
- **æµå¼å¤„ç†**: æ”¯æŒæµå¼æ•°æ®å¤„ç†

### å¯æ‰©å±•æ€§è€ƒè™‘
- **æ°´å¹³æ‰©å±•**: æ”¯æŒå¤šèŠ‚ç‚¹éƒ¨ç½²
- **è´Ÿè½½å‡è¡¡**: å‡è¡¡åˆ†é…è®¡ç®—ä»»åŠ¡
- **å®¹é”™æœºåˆ¶**: å¤„ç†èŠ‚ç‚¹æ•…éšœ

### é²æ£’æ€§è¦æ±‚
- **å¼‚å¸¸å¤„ç†**: å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
- **é™çº§ç­–ç•¥**: åœ¨æ€§èƒ½ä¸‹é™æ—¶çš„å¤‡é€‰æ–¹æ¡ˆ
- **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§ç³»ç»ŸçŠ¶æ€

## ğŸ“ å­¦ä¹ æ€»ç»“

### å…³é”®æ´å¯Ÿ
1. **æ•°å­¦åŸºç¡€**: æ‰å®çš„æ•°å­¦ç†è®ºæ˜¯ç†è§£å’Œæ”¹è¿›ç®—æ³•çš„åŸºç¡€
2. **å·¥ç¨‹å®ç°**: å¥½çš„ç®—æ³•éœ€è¦å¥½çš„å·¥ç¨‹å®ç°
3. **æ€§èƒ½ä¼˜åŒ–**: åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ€§èƒ½å¾€å¾€æ¯”ç²¾åº¦æ›´é‡è¦

### å®è·µè®¡åˆ’
- [ ] å®ç°ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ æµæ°´çº¿
- [ ] ä¼˜åŒ–æ¨¡å‹çš„è®¡ç®—æ•ˆç‡
- [ ] åœ¨Agentç³»ç»Ÿä¸­é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹

## ğŸ¤” æŠ€æœ¯ç–‘é—®

### ç†è®ºé—®é¢˜
- å¦‚ä½•åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Ÿ
- å¦‚ä½•å¤„ç†éå¹³ç¨³ç¯å¢ƒä¸­çš„å­¦ä¹ é—®é¢˜ï¼Ÿ
- å¦‚ä½•è®¾è®¡é€‚åˆAgentç³»ç»Ÿçš„å­¦ä¹ ç®—æ³•ï¼Ÿ

### å®ç°é—®é¢˜
- å¦‚ä½•å¹³è¡¡æ¨¡å‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ï¼Ÿ
- å¦‚ä½•è®¾è®¡é«˜æ•ˆçš„åœ¨çº¿å­¦ä¹ ç®—æ³•ï¼Ÿ
- å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ•°æ®çš„è®­ç»ƒé—®é¢˜ï¼Ÿ

## ğŸ“Š ç›¸å…³é“¾æ¥

**å…±äº«è®¨è®ºåŒº**: [[ğŸ’¬ä¸“é¢˜è®¨è®ºåŒº/Ch01-ç»ªè®º-è®¨è®º]]

**å­¦ä¹ èµ„æº**: [[ğŸ”—å­¦ä¹ èµ„æ–™æ±‡æ€»]]

**ä¸ªäººç¬”è®°**: 
- [[Ch02-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©|ä¸‹ä¸€ç« ]]
- [[ğŸ“šæ ¸å¿ƒæœ¯è¯­è¡¨|æœ¯è¯­è¡¨]]